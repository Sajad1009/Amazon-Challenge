{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we Import the required  Libs for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Amzon_Demo').getOrCreate()\n",
    "from pyspark import SparkContext \n",
    "sc= spark.sparkContext\n",
    "import pandas as pd\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the datasets from HDFS Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+\n",
      "|                 _c0|     _c1| _c2|\n",
      "+--------------------+--------+----+\n",
      "|Sai kha ya her ki...|Positive|null|\n",
      "|           sahi bt h|Positive|null|\n",
      "|         Kya bt hai,|Positive|null|\n",
      "|          Wah je wah|Positive|null|\n",
      "|Are wha kaya bat hai|Positive|null|\n",
      "|  Wah kya baat likhi|Positive|null|\n",
      "|Wha Itni sari khu...|Positive|null|\n",
      "|        Itni khubiya|Positive|null|\n",
      "|Ya allah rehm far...|Positive|null|\n",
      "|Please Everyone A...|Positive|null|\n",
      "+--------------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"Roman1.csv\")\n",
    "df.show(10)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Change the  dataFrame columns names and then drop the unwatned columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Comment: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|Sai kha ya her ki...|Positive|\n",
      "|           sahi bt h|Positive|\n",
      "|         Kya bt hai,|Positive|\n",
      "|          Wah je wah|Positive|\n",
      "|Are wha kaya bat hai|Positive|\n",
      "|  Wah kya baat likhi|Positive|\n",
      "|Wha Itni sari khu...|Positive|\n",
      "|        Itni khubiya|Positive|\n",
      "|Ya allah rehm far...|Positive|\n",
      "|Please Everyone A...|Positive|\n",
      "|Ya mere rab tu br...|Positive|\n",
      "|jaago Pakistani c...|Positive|\n",
      "|kia kia jae .kon ...|Positive|\n",
      "|afsos hota hai ga...|Positive|\n",
      "|  Allah insaaf karey|Positive|\n",
      "|ALLAH MUSALMAN PH...|Positive|\n",
      "|  Je Thik Kaha Right|Positive|\n",
      "|        jee ye to he|Positive|\n",
      "|Maa cheez e ASE h...|Positive|\n",
      "|  Wah wah yeh toh hy|Positive|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newNames  = ['Comment', 'Value', 'None']         \n",
    "dfRenamed = df.toDF(*newNames)\n",
    "columns_to_drop = ['None']\n",
    "df = dfRenamed.drop(*columns_to_drop)\n",
    "df.printSchema()\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Count the null-values and then group them by the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Comment|Value|\n",
      "+-------+-----+\n",
      "|      0|    0|\n",
      "+-------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|               Value|count|\n",
      "+--------------------+-----+\n",
      "|             Neutral| 8913|\n",
      "|            Positive| 5974|\n",
      "|            Negative| 5269|\n",
      "|                null|  336|\n",
      "| jo meine inko de...|    2|\n",
      "| lakin mobile ko ...|    2|\n",
      "| yaha jo bhee hID...|    2|\n",
      "| aur Punjab bar c...|    1|\n",
      "| there were some ...|    1|\n",
      "| director un ke d...|    1|\n",
      "| wanted salt\"\" au...|    1|\n",
      "|                Apne|    1|\n",
      "| Islam khata hay ...|    1|\n",
      "|               laila|    1|\n",
      "| Phantom aur Jagg...|    1|\n",
      "| Ali Zafar ki tar...|    1|\n",
      "| tou wapis karado...|    1|\n",
      "|\"\" k name Saray D...|    1|\n",
      "| wah kya bat hai ...|    1|\n",
      "| par phassti nahi...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "df.groupBy(\"Value\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the Dataframe & Changing the emj to real value to be considerd in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, '😂|😃|😜|😏|😂|😁|♥️|😋|😎|😜|😈|😍|🎁|❤|😁|👍|😘|😌|💖|😚|💋|💕', \" 111 \")  \n",
    "    c = regexp_replace(c, '😒|😣|😓|😒|😭|😡|😠|😖|😭|👊|😱|😞', \" 222 \") \n",
    "   \n",
    " \n",
    "    return c\n",
    "\n",
    "def clean_text_2(c):\n",
    "  \n",
    "   c = regexp_replace(c, \"^rt \", \" \")\n",
    "   c = regexp_replace(c, \"(https?\\://)\\S+\", \" \")\n",
    "   c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \" \")\n",
    "   c = regexp_replace(c, \"[^\\w\\s#@/:%.,_-]\", \" \")\n",
    "   #c = regexp_replace(c, \"[0-9]\", \" \")\n",
    "  \n",
    "\n",
    "   return c\n",
    "   \n",
    "data_1 = dfRenamed.select((clean_text(col(\"Comment\")).alias(\"Comment\")), ((clean_text(col(\"Value\")).alias(\"Value\"))))\n",
    "data_2 = data_1.select((clean_text_2(col(\"Comment\")).alias(\"Comment\")), ((clean_text_2(col(\"Value\")).alias(\"Value\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Check the Categories again and clean the select only the Categories we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Value|count|\n",
      "+--------------------+-----+\n",
      "|             neutral| 8913|\n",
      "|            positive| 5974|\n",
      "|            negative| 5269|\n",
      "|                null|  336|\n",
      "| lakin mobile ko ...|    2|\n",
      "| jo meine inko de...|    2|\n",
      "| yaha jo bhee hid...|    2|\n",
      "| playing by heart...|    1|\n",
      "| stop copying  111  |    1|\n",
      "| there were some ...|    1|\n",
      "| islam khata hay ...|    1|\n",
      "| director un ke d...|    1|\n",
      "| bar asosi aishio...|    1|\n",
      "| tomb raider   me...|    1|\n",
      "| singh is king au...|    1|\n",
      "| sense should pre...|    1|\n",
      "|    hasad na kar    |    1|\n",
      "|     medora lipstick|    1|\n",
      "| 1st time user don t|    1|\n",
      "| tou wapis karado...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "(5269, 2)\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|asif momin hakir ...|negative|\n",
      "|phely jaa kr naha...|negative|\n",
      "|ye to bilkul thk ...|negative|\n",
      "|dukh hi dukh zind...|negative|\n",
      "|or ya assa he hot...|negative|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(5974, 2)\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|sai kha ya her ki...|positive|\n",
      "|           sahi bt h|positive|\n",
      "|         kya bt hai |positive|\n",
      "|          wah je wah|positive|\n",
      "|are wha kaya bat hai|positive|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(8912, 2)\n",
      "+--------------------+-------+\n",
      "|             Comment|  Value|\n",
      "+--------------------+-------+\n",
      "|          hakeqat hy|neutral|\n",
      "|aor aisy bahut km...|neutral|\n",
      "|        jee ye to he|neutral|\n",
      "|hmm jysa kro gy w...|neutral|\n",
      "|ye kia hoa raha h...|neutral|\n",
      "|ghreeb k ghr subh...|neutral|\n",
      "|      ye kia bat hoe|neutral|\n",
      "|ri8 but naseeb ki...|neutral|\n",
      "|  ya post sabka laiy|neutral|\n",
      "|khabi khabi mera ...|neutral|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "data_2.groupBy(\"Value\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(20)\n",
    "\n",
    "df2 = data_2.filter(data_2.Comment. isNotNull() & (data_2.Value == \"negative\")==True)\n",
    "df3 = data_2.filter(data_2.Comment. isNotNull() & (data_2.Value == \"positive\")==True) \n",
    "df4 = data_2.filter(data_2.Comment. isNotNull() & (data_2.Value == \"neutral\")==True)\n",
    "              \n",
    "print((df2.count(), len(df2.columns)))\n",
    "df2.show(5)\n",
    "print((df3.count(), len(df3.columns)))\n",
    "df3.show(5)\n",
    "print((df4.count(), len(df4.columns)))\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting all the data and make it as one dataframe \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5269, 2)\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|asif momin hakir ...|negative|\n",
      "|phely jaa kr naha...|negative|\n",
      "|ye to bilkul thk ...|negative|\n",
      "|dukh hi dukh zind...|negative|\n",
      "|or ya assa he hot...|negative|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(5974, 2)\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|sai kha ya her ki...|positive|\n",
      "|           sahi bt h|positive|\n",
      "|         kya bt hai |positive|\n",
      "|          wah je wah|positive|\n",
      "|are wha kaya bat hai|positive|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(8912, 2)\n",
      "+--------------------+-------+\n",
      "|             Comment|  Value|\n",
      "+--------------------+-------+\n",
      "|          hakeqat hy|neutral|\n",
      "|aor aisy bahut km...|neutral|\n",
      "|        jee ye to he|neutral|\n",
      "|hmm jysa kro gy w...|neutral|\n",
      "|ye kia hoa raha h...|neutral|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(20155, 2)\n",
      "+--------------------+--------+\n",
      "|             Comment|   Value|\n",
      "+--------------------+--------+\n",
      "|asif momin hakir ...|negative|\n",
      "|phely jaa kr naha...|negative|\n",
      "|ye to bilkul thk ...|negative|\n",
      "|dukh hi dukh zind...|negative|\n",
      "|or ya assa he hot...|negative|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((df2.count(), len(df2.columns)))\n",
    "df2.show(5)\n",
    "print((df3.count(), len(df3.columns)))\n",
    "df3.show(5)\n",
    "print((df4.count(), len(df4.columns)))\n",
    "df4.show(5)\n",
    "\n",
    "df5 = df2.union(df3)\n",
    "df_final=df5.union(df4)\n",
    "\n",
    "print((df_final.count(), len(df_final.columns)))\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can tokenize the dataframe and delete the less than 3 char tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19895, 2)\n",
      "+--------+--------------------+\n",
      "|   Value|             Comment|\n",
      "+--------+--------------------+\n",
      "|negative|[asif, momin, hak...|\n",
      "|negative|[phely, jaa, naha...|\n",
      "|negative|[bilkul, thk, kah...|\n",
      "|negative|[dukh, dukh, zind...|\n",
      "|negative|[assa, hotta, jas...|\n",
      "|negative|[twadi, bahan, no...|\n",
      "|negative|[agree, main, sth...|\n",
      "|negative|[ghareeb, insan, ...|\n",
      "|negative|[plz, police, tha...|\n",
      "|negative|[sindh, police, t...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Comment\", outputCol=\"words_token\", pattern=\"\\\\W\")\n",
    "df_words_token = regexTokenizer.transform(df_final).select('Value', 'words_token')\n",
    "\n",
    "df222 = df_words_token.withColumn(\"Comment\", f.expr(\"filter(words_token, x -> not(length(x) < 3))\")).where(f.size(f.col(\"Comment\")) > 0).drop(\"words_token\")\n",
    "print((df222.count(), len(df222.columns)))\n",
    "df222.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets prepare our dataframe to our machine learning models by removing  unwanted words and doing  bags of words\n",
    "#### Then we will create pipeline to push the dataset into it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "|   Value|             Comment|            filtered|            features|label|\n",
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "|negative|[asif, momin, hak...|[asif, momin, hak...|(8753,[1,10,82,95...|  2.0|\n",
      "|negative|[phely, jaa, naha...|[phely, jaa, naha...|(8753,[193,1392,2...|  2.0|\n",
      "|negative|[bilkul, thk, kah...|[bilkul, thk, kah...|(8753,[2,75,123,1...|  2.0|\n",
      "|negative|[dukh, dukh, zind...|[dukh, dukh, zind...|(8753,[0,1315],[1...|  2.0|\n",
      "|negative|[assa, hotta, jas...|[assa, hotta, jas...|(8753,[5,28,108,1...|  2.0|\n",
      "|negative|[twadi, bahan, no...|[twadi, bahan, no...|(8753,[1,2,4,5,6,...|  2.0|\n",
      "|negative|[agree, main, sth...|[agree, main, sth...|(8753,[9,12,124,3...|  2.0|\n",
      "|negative|[ghareeb, insan, ...|[ghareeb, insan, ...|(8753,[2,7,12,53,...|  2.0|\n",
      "|negative|[plz, police, tha...|[plz, police, tha...|(8753,[92,190,115...|  2.0|\n",
      "|negative|[sindh, police, t...|[sindh, police, t...|(8753,[7,15,33,81...|  2.0|\n",
      "|negative|[pakistan, tabah,...|[pakistan, tabah,...|(8753,[1,15,160,5...|  2.0|\n",
      "|negative|[kia, hogya, poli...|[kia, hogya, poli...|(8753,[9,19,71,15...|  2.0|\n",
      "|negative|[nawaz, sharif, p...|[nawaz, sharif, p...|(8753,[68,115,137...|  2.0|\n",
      "|negative|[police, walon, m...|[police, walon, m...|(8753,[35,68,190,...|  2.0|\n",
      "|negative|[police, wala, ak...|[police, wala, ak...|(8753,[9,69,98,12...|  2.0|\n",
      "|negative|[police, per, lan...|[police, per, lan...|(8753,[16,30,33,1...|  2.0|\n",
      "|negative|[mary, khyal, pol...|[mary, khyal, pol...|(8753,[190,977,28...|  2.0|\n",
      "|negative|[coda, kro, polic...|[coda, kro, polic...|(8753,[184,190,44...|  2.0|\n",
      "|negative|[mere, sat, bhi, ...|[mere, sat, bhi, ...|(8753,[4,6,22,50,...|  2.0|\n",
      "|negative|[bherwe, tujhe, a...|[bherwe, tujhe, a...|(8753,[25,65,119,...|  2.0|\n",
      "+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "\n",
    "\n",
    "# stop words\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"Comment\", outputCol=\"filtered\").setStopWords(stopwords)\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=3)\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"Value\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df222)\n",
    "dataset = pipelineFit.transform(df222)\n",
    "dataset.show(20)\n",
    "# bag of words count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Lets divide our datasets into trainsets and testsets for testing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 15940\n",
      "Test Dataset Count: 3955\n"
     ]
    }
   ],
   "source": [
    "# split the data and count them \n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets test our models with the algorithms we got in pyspark and do Multiclass-Classification-Evaluator\n",
    "#### First Lets start with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                       Comment|   Value|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|[sonal, chauhanne, telugu, ...|positive|[0.9948291268712804,0.00481...|  1.0|       0.0|\n",
      "|[ohh, got, real, competitio...| neutral|[0.9814307339348136,0.01152...|  0.0|       0.0|\n",
      "|[1973, mein, harvard, unive...| neutral|[0.9779763800686536,0.01588...|  0.0|       0.0|\n",
      "|[brigadier, hesiyat, unhon,...| neutral|[0.9759593428164509,0.01918...|  0.0|       0.0|\n",
      "|[sri, lanka, batsman, kumar...|positive|[0.9754151876781293,0.01338...|  1.0|       0.0|\n",
      "|[jab, guzishta, aik, saal, ...| neutral|[0.9712226647832201,0.02275...|  0.0|       0.0|\n",
      "|[primary, school, number, w...| neutral|[0.9657880870496699,0.01650...|  0.0|       0.0|\n",
      "|[chunache, pir, pagara, pak...| neutral|[0.9655016609864637,0.01896...|  0.0|       0.0|\n",
      "|[tahum, maut, years, bad, a...| neutral|[0.9622694992836397,0.01044...|  0.0|       0.0|\n",
      "|[tasaneef, mein, sirf, book...| neutral|[0.9620653338915889,0.02257...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6238593500842845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Comment\",\"Value\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets add HashingTF and IDF Methodes to our  LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                       Comment|   Value|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|[begum, liaquat, batan, far...| neutral|[0.9850723268672922,0.00944...|  0.0|       0.0|\n",
      "|[1973, mein, harvard, unive...| neutral|[0.9674589746691525,0.02654...|  0.0|       0.0|\n",
      "|     [smja, inhy, momina, kis]|negative|[0.9592677565775155,0.01254...|  2.0|       0.0|\n",
      "|[zulfiqar, ali, bhutto, nay...| neutral|[0.9569974927345575,0.01956...|  0.0|       0.0|\n",
      "|     [kiya, kehh, sakte, henn]| neutral|[0.952319260852304,0.020049...|  0.0|       0.0|\n",
      "|[rice, isqaat, hamal, khila...|negative|[0.9505715375953411,0.01304...|  2.0|       0.0|\n",
      "|[come, tuhadee, phuphee, lu...|negative|[0.9495398376280736,0.02369...|  2.0|       0.0|\n",
      "|[helen, keller, 1880, mein,...| neutral|[0.9487525020127384,0.01849...|  0.0|       0.0|\n",
      "|[anab, altaf, hussain, nay,...|positive|[0.9468301617032723,0.03737...|  1.0|       0.0|\n",
      "|[zilla, aur, tehsil, counce...|negative|[0.9462576116635412,0.02488...|  2.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5884725888089297"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(df222)\n",
    "dataset = pipelineFit.transform(df222)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Comment\",\"Value\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets do CrossOver Methodes hopfully  we will add some more accurcy to our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6219636491887839"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(df222)\n",
    "dataset = pipelineFit.transform(df222)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### its dose not  change that much. Lets try another classfication algorithem - NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                       Comment|   Value|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|[zulfiqar, ali, bhutto, nay...| neutral|[0.9999674360403801,3.24917...|  0.0|       0.0|\n",
      "|[phupo, thanks, bolin, meri...|positive|[0.9998634583068682,9.71751...|  1.0|       0.0|\n",
      "|[meri, tarf, apny, parents,...| neutral|[0.9997063494520431,1.27534...|  0.0|       0.0|\n",
      "|[inshallah, hum, dono, kaam...|positive|[0.9996429337791103,1.87496...|  1.0|       0.0|\n",
      "|[pictures, screenshot, maar...|negative|[0.9995571475107323,2.18137...|  2.0|       0.0|\n",
      "|[hahahahahah, 111, 111, 111...| neutral|[0.9995073048266958,3.93420...|  0.0|       0.0|\n",
      "|[111, 111, 111, thnkew, den...| neutral|[0.999439563014437,1.928399...|  0.0|       0.0|\n",
      "|[uncle, anty, thanks, kehna...| neutral|[0.9992770363538133,5.75694...|  0.0|       0.0|\n",
      "|[hahaha, new, wala, pehno, ...| neutral|[0.9991893573978425,1.51595...|  0.0|       0.0|\n",
      "|[111, bhi, thank, you, bol,...| neutral|[0.9990677568758084,6.12351...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.648457283205311"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=5)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Comment\",\"Value\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "                    \n",
    "    \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice we get some improvements in our model. Considering the given data we have in roman urdu 65 accuracy its good for multiclassfiction text sentiment problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limitation of the data used \n",
    "# 1- The data we used was entirely missy contain numbers emojis and other non-english characters\n",
    "# 2- The data used for foreign Language written in english characters which make it difficult for us to use techniques such as\n",
    "# StopWordRemover and Normalization such as stemming and lemmatization\n",
    "# 3- The datasets collected  not accurate. We find lots of negative comments contain happy words such as ( lol, hahhahha,\n",
    "# happy emoj, and postive words in urdu language )\n",
    "# 4- The datasets unbalanced (6000 postive comments, 6000 negative comments and around 8000 neutral comments ) \n",
    "# however there is lots of numbers and none-sense datasets (Comments) in netural comments...after clean it carfully\n",
    "# the data become somehow close in terms of numbers (Balanced) becasue lot of null datasets deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
